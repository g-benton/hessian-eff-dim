{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch import nn\n",
    "\n",
    "import hess\n",
    "import hess.net_utils as net_utils\n",
    "import hess.utils as utils\n",
    "from hess.nets import MaskedNetLinear, SubNetLinear\n",
    "# from hess.nets import MaskedLayerLinear, SubLayerLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twospirals(n_points, noise=.5, random_state=920):\n",
    "    \"\"\"\n",
    "     Returns the two spirals dataset.\n",
    "    \"\"\"\n",
    "    n = np.sqrt(np.random.rand(n_points,1)) * 600 * (2*np.pi)/360\n",
    "    d1x = -1.5*np.cos(n)*n + np.random.randn(n_points,1) * noise\n",
    "    d1y =  1.5*np.sin(n)*n + np.random.randn(n_points,1) * noise\n",
    "    return (np.vstack((np.hstack((d1x,d1y)),np.hstack((-d1x,-d1y)))),\n",
    "            np.hstack((np.zeros(n_points),np.ones(n_points))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Setting prune rate of network to 0.5\n",
      "==> Setting prune rate of sequential.0 to 0.5\n",
      "==> Setting prune rate of sequential.2 to 0.5\n",
      "==> Setting prune rate of sequential.4 to 0.5\n",
      "==> Setting prune rate of sequential.6 to 0.5\n",
      "==> Setting prune rate of sequential.8 to 0.5\n",
      "==> Setting prune rate of sequential.10 to 0.5\n",
      "==> Setting prune rate of sequential.12 to 0.5\n",
      "=> Freezing model weights\n",
      "==> No gradient to sequential.0.weight\n",
      "==> No gradient to sequential.2.weight\n",
      "==> No gradient to sequential.4.weight\n",
      "==> No gradient to sequential.6.weight\n",
      "==> No gradient to sequential.8.weight\n",
      "==> No gradient to sequential.10.weight\n",
      "==> No gradient to sequential.12.weight\n",
      "==> Applied Weights\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "==> Applied Mask\n",
      "tensor([1., 1., 1.,  ..., 0., 1., 0.], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "X, Y = twospirals(500, noise=1.3)\n",
    "train_x = torch.FloatTensor(X)\n",
    "train_y = torch.FloatTensor(Y).unsqueeze(-1)\n",
    "\n",
    "###################################\n",
    "## Set up nets and match weights ##\n",
    "###################################\n",
    "\n",
    "n_hidden = 5\n",
    "width = 15\n",
    "\n",
    "subnet_model = SubNetLinear(in_dim=2, out_dim=1, n_layers=n_hidden, k=width, bias=False)\n",
    "masked_model = MaskedNetLinear(in_dim=2, out_dim=1, n_layers=n_hidden, k=width, bias=False)\n",
    "\n",
    "hess.net_utils.set_model_prune_rate(subnet_model, 0.5)\n",
    "hess.net_utils.freeze_model_weights(subnet_model)\n",
    "\n",
    "weights = net_utils.get_weights_from_subnet(subnet_model)\n",
    "\n",
    "net_utils.apply_weights(masked_model, weights)\n",
    "mask = net_utils.get_mask_from_subnet(subnet_model)\n",
    "net_utils.apply_mask(masked_model, mask)\n",
    "mask = utils.flatten(mask)\n",
    "print(mask)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.cuda.set_device(3)\n",
    "    train_x, train_y = train_x.cuda(), train_y.cuda()\n",
    "    subnet_model = subnet_model.cuda()\n",
    "    masked_model = masked_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "==> Applied Mask\n",
      "numpars is:  588\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "tensor(0.6930, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6929, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6928, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6928, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6927, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6927, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6926, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6925, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6924, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6921, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "==> Applied Mask\n",
      "numpars is:  588\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "tensor(0.6917, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6915, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6907, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6903, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6902, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6900, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6896, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6886, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6884, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6884, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "==> Applied Mask\n",
      "numpars is:  588\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "tensor(0.6882, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6880, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6880, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6880, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6875, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6875, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6873, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6871, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6871, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6870, device='cuda:3', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "cuda:3\n",
      "==> Applied Mask\n",
      "numpars is:  588\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n",
      "finished a hvp\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d8a3927b48c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         eigs = utils.get_hessian_eigs(loss_func, masked_model, mask=mask,\n\u001b[1;32m     26\u001b[0m                                       \u001b[0mn_eigs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_eigs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                                       train_y=train_y)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0meigs_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hess_truck/hess/utils.py\u001b[0m in \u001b[0;36mget_hessian_eigs\u001b[0;34m(loss, model, mask, use_cuda, n_eigs, train_x, train_y, loader, evals)\u001b[0m\n\u001b[1;32m    204\u001b[0m         qmat, tmat = lanczos_tridiag(hvp, n_eigs, dtype=dtype,\n\u001b[1;32m    205\u001b[0m                                   device=device, matrix_shape=(numpars,\n\u001b[0;32m--> 206\u001b[0;31m                                   numpars))\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0meigs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanczos_tridiag_to_diag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/utils/lanczos.py\u001b[0m in \u001b[0;36mlanczos_tridiag\u001b[0;34m(matmul_closure, max_iter, dtype, device, matrix_shape, batch_shape, init_vecs, num_init_vecs, tol)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# Compute next alpha value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mr_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatmul_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_curr_vec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mq_prev_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0malpha_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_curr_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Copy over to t_mat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hess_truck/hess/utils.py\u001b[0m in \u001b[0;36mhvp\u001b[0;34m(rhs)\u001b[0m\n\u001b[1;32m    186\u001b[0m             eval_hess_vec_prod(padded_rhs, net=model,\n\u001b[1;32m    187\u001b[0m                                \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                                targets=train_y, dataloader=loader, use_cuda=use_cuda)\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0mfull_hvp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradtensor_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_bn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0msliced_hvp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_hvp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hess_truck/hess/utils.py\u001b[0m in \u001b[0;36meval_hess_vec_prod\u001b[0;34m(vec, net, criterion, inputs, targets, dataloader, use_cuda)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mgrad_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/home/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hess_truck/hess/nets/linear_subnets.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/home/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/home/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######################\n",
    "## Train the Subnet ##\n",
    "######################\n",
    "\n",
    "optimizer = torch.optim.Adam(subnet_model.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "eigs_every = 10\n",
    "n_eigs = 100\n",
    "eigs_out = []\n",
    "\n",
    "for step in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = subnet_model(train_x)\n",
    "\n",
    "    loss=loss_func(outputs,train_y)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % eigs_every == 0:\n",
    "        mask = net_utils.get_mask_from_subnet(subnet_model)\n",
    "        net_utils.apply_mask(masked_model, mask)\n",
    "        mask = utils.flatten(mask)\n",
    "\n",
    "        eigs = utils.get_hessian_eigs(loss_func, masked_model, mask=mask,\n",
    "                                      n_eigs=n_eigs, train_x=train_x,\n",
    "                                      train_y=train_y)\n",
    "\n",
    "        eigs_out.append(eigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"./saved-subnet-hessian/\"\n",
    "fname = \"subnet_eigs.pkl\"\n",
    "\n",
    "with open(fpath + fname, 'wb') as f:\n",
    "    pickle.dump(eigs_out, f)\n",
    "\n",
    "fname = \"subnet_model.pt\"\n",
    "torch.save(subnet_model.state_dict(), fpath + fname)\n",
    "\n",
    "fname = \"masked_model.pt\"\n",
    "torch.save(masked_model.state_dict(), fpath + fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
