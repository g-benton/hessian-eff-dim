{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch import nn\n",
    "\n",
    "import hess\n",
    "import hess.net_utils as net_utils\n",
    "from hess.nets import MaskedSubnetLinear\n",
    "from hess.nets import SubnetLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twospirals(n_points, noise=.5, random_state=920):\n",
    "    \"\"\"\n",
    "     Returns the two spirals dataset.\n",
    "    \"\"\"\n",
    "    n = np.sqrt(np.random.rand(n_points,1)) * 600 * (2*np.pi)/360\n",
    "    d1x = -1.5*np.cos(n)*n + np.random.randn(n_points,1) * noise\n",
    "    d1y =  1.5*np.sin(n)*n + np.random.randn(n_points,1) * noise\n",
    "    return (np.vstack((np.hstack((d1x,d1y)),np.hstack((-d1x,-d1y)))),\n",
    "            np.hstack((np.zeros(n_points),np.ones(n_points))))\n",
    "\n",
    "\n",
    "class Subnet(nn.Module):\n",
    "    \"\"\"\n",
    "    Small MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, k=16,\n",
    "                 n_layers=5, kernel_size=3,\n",
    "                activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        module = nn.ModuleList()\n",
    "\n",
    "        module.append(SubnetLinear(in_dim, k))\n",
    "        module.append(activation)\n",
    "\n",
    "        for ll in range(n_layers-1):\n",
    "            module.append(SubnetLinear(k, k))\n",
    "            module.append(activation)\n",
    "\n",
    "        module.append(SubnetLinear(k, k))\n",
    "        module.append(activation)\n",
    "        module.append(SubnetLinear(k, out_dim))\n",
    "        self.sequential = nn.Sequential(*module)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "class MaskNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Small MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, k=16,\n",
    "                 n_layers=5, kernel_size=3,\n",
    "                activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        module = nn.ModuleList()\n",
    "\n",
    "        module.append(MaskedLinear(in_dim, k))\n",
    "        module.append(activation)\n",
    "\n",
    "        for ll in range(n_layers-1):\n",
    "            module.append(MaskedLinear(k, k))\n",
    "            module.append(activation)\n",
    "\n",
    "        module.append(MaskedLinear(k, k))\n",
    "        module.append(activation)\n",
    "        module.append(MaskedLinear(k, out_dim))\n",
    "        self.sequential = nn.Sequential(*module)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.sequential(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = twospirals(500, noise=1.3)\n",
    "train_x = torch.FloatTensor(X)\n",
    "train_y = torch.FloatTensor(Y).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
